{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmEuesJbb3LF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"merged_df.csv\")\n",
        "\n",
        "feature_cols = [\n",
        "    \"UNIXTime\", \"Temperature\", \"Pressure\", \"Humidity\",\n",
        "    \"WindDirection(Degrees)\", \"Speed\", \"latitude\", \"longitude\",\n",
        "    \"ws10m\", \"t2m\"\n",
        "]\n",
        "target_cols = [\"Radiation\", \"allsky_sfc_sw_dwn\"]\n",
        "\n",
        "X = df[feature_cols]\n",
        "Y = df[target_cols]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train_raw, X_test_raw, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
        "X_test_scaled = scaler.transform(X_test_raw)\n",
        "\n",
        "#pca to find compoenents giving us 95 percnet monitor\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "\n",
        "n_original = X_train_raw.shape[1]\n",
        "n_pca = X_train_pca.shape[1]\n",
        "\n",
        "print(f\"Original features: {n_original}\")\n",
        "print(f\"Components for 95% variance: {n_pca}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Based on the results, we will use the entire dataset for modeling as all features contribute to 95 percent variance.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train_scaled\n",
        "X_test = X_test_scaled\n",
        "y_train = Y_train.iloc[:, 0]\n",
        "y_test = Y_test.iloc[:, 0]\n",
        "\n",
        "#code for error for linreg, degree 1\n",
        "linear_regression_model = LinearRegression()\n",
        "linear_regression_model.fit(X_train, y_train)\n",
        "\n",
        "linear_regression_prediction_training = linear_regression_model.predict(X_train)\n",
        "linear_regression_prediction_testing = linear_regression_model.predict(X_test)\n",
        "\n",
        "error_linear_train = mean_squared_error(y_train, linear_regression_prediction_training)\n",
        "error_linear_test = mean_squared_error(y_test, linear_regression_prediction_testing)\n",
        "print(\"Below are the outputs for  Linear Regression\")\n",
        "print(\"Test error: \", np.sqrt(error_linear_train))\n",
        "print(\"Training error: \", np.sqrt(error_linear_test))\n",
        "\n",
        "#code for polynomial linreg\n",
        "poly_model = PolynomialFeatures(degree =2)\n",
        "X_train_polynomial = poly_model.fit_transform(X_train)\n",
        "X_test_polynomial = poly_model.transform(X_test)\n",
        "polynomial_model = LinearRegression()\n",
        "polynomial_model.fit(X_train_polynomial, y_train)\n",
        "polynomial_regression_prediction_training = polynomial_model.predict(X_train_polynomial)\n",
        "polynomial_regression_prediction_testing = polynomial_model.predict(X_test_polynomial)\n",
        "\n",
        "error_polynomial_train = mean_squared_error(y_train, polynomial_regression_prediction_training)\n",
        "error_polynomial_test = mean_squared_error(y_test, polynomial_regression_prediction_testing)\n",
        "print(\"Below are the outputs for polynomial Linear Regression\")\n",
        "print(\"Train error: \", np.sqrt(error_polynomial_train))\n",
        "print(\"Test error: \", np.sqrt(error_polynomial_test))\n",
        "\n",
        "#code for ridge-based polynomial regression\n",
        "\n",
        "ridge_model = Ridge(alpha = 100.0)\n",
        "ridge_model.fit(X_train_polynomial, y_train)\n",
        "ridge_regression_prediction_training = ridge_model.predict(X_train_polynomial)\n",
        "ridge_regression_prediction_testing = ridge_model.predict(X_test_polynomial)\n",
        "\n",
        "error_ridge_train = mean_squared_error(y_train, ridge_regression_prediction_training)\n",
        "error_ridge_test = mean_squared_error(y_test, ridge_regression_prediction_testing)\n",
        "print(\"Below are the outputs for Ridge-regularized Linear Regression\")\n",
        "print(\"Train error: \", np.sqrt(error_ridge_train))\n",
        "print(\"Test error: \", np.sqrt(error_ridge_test))\n",
        "\n",
        "#code for lasso-based polynomial regression\n",
        "\n",
        "lasso_model = Lasso(alpha = .0001, max_iter = 100000)\n",
        "lasso_model.fit(X_train_polynomial, y_train)\n",
        "lasso_regression_prediction_training = ridge_model.predict(X_train_polynomial)\n",
        "lasso_regression_prediction_testing = ridge_model.predict(X_test_polynomial)\n",
        "\n",
        "error_lasso_train = mean_squared_error(y_train, ridge_regression_prediction_training)\n",
        "error_lasso_test = mean_squared_error(y_test, ridge_regression_prediction_testing)\n",
        "print(\"Below are the outputs for lasso-regularized Linear Regression\")\n",
        "print(\"Train error: \", np.sqrt(error_lasso_train))\n",
        "print(\"Test error: \", np.sqrt(error_lasso_test))\n",
        "\n",
        "#graphing polynomial, we deemed it best\n",
        "plt.figure(figsize = (20,5))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.scatter(y_train, polynomial_regression_prediction_training, alpha = .5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw =2)\n",
        "plt.xlabel(\"Actual values\")\n",
        "plt.ylabel(\"Predicted values\")\n",
        "plt.title(\"Actual vs Predicted Values for Polynomial Model\")\n",
        "\n",
        "residuals = y_test - polynomial_regression_prediction_testing\n",
        "plt.subplot(1,3,2)\n",
        "plt.scatter(polynomial_regression_prediction_testing, residuals, alpha = .5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw =2)\n",
        "plt.xlabel(\"Predicted values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residual Plot for Polynomial Model\")\n",
        "\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "coef_values = polynomial_model.coef_\n",
        "#just to make it more viewable in colab UI, we restricted to 8 features\n",
        "coef_series = pd.Series(coef_values, index = poly_model.get_feature_names_out(X.columns))\n",
        "coef_series_shortened = coef_series[:8]\n",
        "coef_series_shortened.plot(kind = 'barh')\n",
        "plt.tight_layout()\n",
        "plt.title(\"Feature Importance of Polynomial Model\")\n",
        "plt.xlabel(\"Coefficient Value\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8g1Lj8n-cU8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "X_train_nn = np.array(X_train_scaled)\n",
        "X_test_nn = np.array(X_test_scaled)\n",
        "y_train_nn = Y_train.iloc[:, 0].values\n",
        "y_test_nn = Y_test.iloc[:, 0].values\n",
        "\n",
        "node_options = [16, 64, 256, 1024, 2048, 4096, 8192, 16384, 32768]\n",
        "\n",
        "\n",
        "results = {\n",
        "    \"nodes\": [],\n",
        "    \"avg_val_rmse\": [],\n",
        "    \"avg_train_rmse\": []\n",
        "}\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=2, shuffle=True, random_state=41)\n",
        "\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "for node_count in node_options:\n",
        "    fold_val_rmse = []\n",
        "    fold_train_rmse = []\n",
        "\n",
        "\n",
        "    for train_index, val_index in kf.split(X_train_nn):\n",
        "        X_fold_train = X_train_nn[train_index]\n",
        "        X_fold_val = X_train_nn[val_index]\n",
        "        y_fold_train = y_train_nn[train_index]\n",
        "        y_fold_val = y_train_nn[val_index]\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(X_train_nn.shape[1],)))\n",
        "        model.add(Dense(node_count, activation=\"relu\"))\n",
        "        model.add(Dense(1, activation=\"linear\"))\n",
        "\n",
        "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "        model.fit(\n",
        "            X_fold_train,\n",
        "            y_fold_train,\n",
        "            validation_data=(X_fold_val, y_fold_val),\n",
        "            epochs=37,\n",
        "            batch_size=3,\n",
        "            callbacks=[early_stop],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        y_pred_val = model.predict(X_fold_val, verbose=0).flatten()\n",
        "        val_rmse = np.sqrt(mean_squared_error(y_fold_val, y_pred_val))\n",
        "        fold_val_rmse.append(val_rmse)\n",
        "\n",
        "\n",
        "        y_pred_train = model.predict(X_fold_train, verbose=0).flatten()\n",
        "        train_rmse = np.sqrt(mean_squared_error(y_fold_train, y_pred_train))\n",
        "        fold_train_rmse.append(train_rmse)\n",
        "\n",
        "\n",
        "    avg_val_rmse = np.mean(fold_val_rmse)\n",
        "    avg_train_rmse = np.mean(fold_train_rmse)\n",
        "\n",
        "    results[\"nodes\"].append(node_count)\n",
        "    results[\"avg_val_rmse\"].append(avg_val_rmse)\n",
        "    results[\"avg_train_rmse\"].append(avg_train_rmse)\n",
        "\n",
        "    print(node_count, \"|\", round(avg_train_rmse, 4), \"|\", round(avg_val_rmse, 4))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(results[\"nodes\"], results[\"avg_val_rmse\"], marker=\"o\", label=\"CV Validation Error (Avg)\")\n",
        "plt.plot(results[\"nodes\"], results[\"avg_train_rmse\"], marker=\"o\", linestyle=\"--\", label=\"CV Training Error (Avg)\")\n",
        "\n",
        "plt.title(\"5-Fold Cross Validation: Model Complexity vs. Error\")\n",
        "plt.xlabel(\"Number of Nodes\")\n",
        "plt.ylabel(\"Average RMSE\")\n",
        "plt.xscale(\"log\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "final_model = Sequential()\n",
        "final_model.add(Input(shape=(X_train_nn.shape[1],)))\n",
        "final_model.add(Dense(32768 , activation=\"relu\"))\n",
        "final_model.add(Dense(1, activation=\"linear\"))\n",
        "final_model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "\n",
        "final_model.fit(\n",
        "    X_train_nn,\n",
        "    y_train_nn,\n",
        "    validation_split=0.2,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "y_final_pred = final_model.predict(X_test_nn, verbose=0).flatten()\n",
        "final_test_rmse = np.sqrt(mean_squared_error(y_test_nn, y_final_pred))\n",
        "final_r2 = r2_score(y_test_nn, y_final_pred)\n",
        "\n",
        "print(\"Final Test error:\", final_test_rmse)\n",
        "print(\"Final Test r2:  \", final_r2)\n"
      ],
      "metadata": {
        "id": "cO01xpsS9y63"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}